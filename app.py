import os
import sys
import subprocess
import pandas as pd
from flask import Flask, render_template, request, redirect, url_for, flash, jsonify
from werkzeug.utils import secure_filename
import json
import joblib # <-- Add import for joblib
import warnings # <-- Add import for warnings
import numpy as np # <-- Add import for numpy

# --- Configuration ---
# This path is the single source of truth
BASE_FOLDER = r"D:\FlaskDev"
INPUT_FOLDER = os.path.join(BASE_FOLDER, "data", "excel_folder")
PROCESSED_FOLDER = os.path.join(BASE_FOLDER, "data", "processed")
LOGS_FOLDER = os.path.join(BASE_FOLDER, "logs")

# Files generated by the pipeline
CLEANED_CSV = os.path.join(PROCESSED_FOLDER, "master_dataset_cleaned.csv")
EXCEL_LOG = os.path.join(LOGS_FOLDER, "converted_files.txt")
MODEL_FILE = os.path.join(LOGS_FOLDER, "random_forest_model.pkl") # <-- Path to model
CASE_DICT_FILE = os.path.join(LOGS_FOLDER, "case_dictionary.json") # <-- Path to case dict

ALLOWED_EXTENSIONS = {'xlsx'}

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = INPUT_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max upload
app.secret_key = 'your-secret-key-goes-here' # Change this

# --- Helper Functions --- (allowed_file, get_dashboard_stats)
# (These functions are unchanged)

def allowed_file(filename):
    """Checks if the file extension is allowed (Excel only)"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def get_dashboard_stats():
    """Calculates the stats for the dashboard"""
    stats = {
        'total_files': 0,
        'total_records': 0,
        'success_rate': 100.0,
        'processing': 0 
    }
    os.makedirs(INPUT_FOLDER, exist_ok=True)
    os.makedirs(PROCESSED_FOLDER, exist_ok=True)
    os.makedirs(LOGS_FOLDER, exist_ok=True)
    try:
        if os.path.exists(EXCEL_LOG):
            with open(EXCEL_LOG, "r") as f:
                processed_count = len(f.read().splitlines())
        stats['total_files'] = processed_count
    except Exception:
        stats['total_files'] = 0
    try:
        df = pd.read_csv(CLEANED_CSV)
        stats['total_records'] = len(df)
    except (FileNotFoundError, pd.errors.EmptyDataError):
        stats['total_records'] = 0
    except Exception as e:
        print(f"Error reading cleaned CSV: {e}")
        stats['total_records'] = 0
    if stats['total_files'] == 0:
        stats['success_rate'] = 0.0
    else:
        stats['success_rate'] = 100.0
    return stats


# --- Flask Routes ---

@app.route('/')
def index():
    """Renders the main dashboard page with updated stats."""
    stats = get_dashboard_stats()
    return render_template('index.html', stats=stats)


@app.route('/upload', methods=['POST'])
def upload_file():
    """Handles file upload and triggers the data pipeline ONLY."""
    
    files = request.files.getlist('file')
    
    if not files or files[0].filename == '':
        flash('No selected files', 'error')
        return redirect(url_for('index'))

    # (File saving logic is unchanged)
    processed_files_log = set()
    try:
        if os.path.exists(EXCEL_LOG):
            with open(EXCEL_LOG, "r") as f:
                processed_files_log = set(line.strip() for line in f.readlines())
    except Exception as e:
        print(f"Warning: Could not read log file {EXCEL_LOG}. {e}")

    new_files_saved = []
    skipped_files_duplicate = []
    invalid_type_files = []

    for file in files:
        filename = secure_filename(file.filename)
        if filename == '': continue
        if not allowed_file(filename):
            invalid_type_files.append(filename)
            continue
        if filename in processed_files_log:
            skipped_files_duplicate.append(filename)
            continue
        try:
            save_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            file.save(save_path)
            new_files_saved.append(filename)
        except Exception as e:
            print(f"Error saving file {filename}: {e}")
            invalid_type_files.append(f"{filename} (Save Error)")

    # (Pipeline run logic is unchanged)
    pipeline_error = None
    if new_files_saved:
        try:
            print("--- [APP] Running data pipeline ---")
            pipeline_path = os.path.join(BASE_FOLDER, 'pipeline.py') 
            proc_env = os.environ.copy()
            proc_env['PYTHONIOENCODING'] = 'utf-8'
            result = subprocess.run(
                [sys.executable, pipeline_path, BASE_FOLDER],
                capture_output=True, text=True, check=True,
                encoding='utf-8', env=proc_env
            )
            print("--- [APP] Pipeline STDOUT:", result.stdout)
            print("--- [APP] Pipeline STDERR:", result.stderr)
            print("--- [APP] Pipeline complete. ---")
        except subprocess.CalledProcessError as e:
            print(f"--- [APP] Pipeline failed with code {e.returncode} ---")
            print("Pipeline STDOUT:", e.stdout)
            print("Pipeline STDERR:", e.stderr)
            pipeline_error = f'Data pipeline failed. See console.'
        
    # (Flash message logic is unchanged)
    if pipeline_error:
        flash(f'Uploaded {len(new_files_saved)} file(s), but the pipeline failed: {pipeline_error}', 'error')
    elif new_files_saved:
        flash(f'Successfully processed {len(new_files_saved)} new file(s). Data is ready for training.', 'success')
    if skipped_files_duplicate:
        flash(f'Skipped {len(skipped_files_duplicate)} file(s) (already processed): {", ".join(skipped_files_duplicate)}', 'warning')
    if invalid_type_files:
        flash(f'Skipped {len(invalid_type_files)} file(s) (invalid type or save error).', 'warning')
    if not new_files_saved and not skipped_files_duplicate and not invalid_type_files:
        flash('No files were selected.', 'error')
    elif not new_files_saved:
        flash('No new files to process.', 'info')
            
    return redirect(url_for('index'))


@app.route('/train', methods=['POST'])
def train_model():
    """Triggers the model training script."""
    
    # (This route is unchanged)
    print("--- [APP] Received request to train model ---")
    try:
        train_script_path = os.path.join(BASE_FOLDER, 'train_model.py')
        proc_env = os.environ.copy()
        proc_env['PYTHONIOENCODING'] = 'utf-8'
        result = subprocess.run(
            [sys.executable, train_script_path, BASE_FOLDER],
            capture_output=True, text=True, check=True,
            encoding='utf-8', env=proc_env
        )
        print("--- [APP] Training script STDOUT:", result.stdout)
        print("--- [APP] Training script STDERR:", result.stderr)
        
        try:
            train_results = json.loads(result.stdout)
            if "error" in train_results:
                flash(f"Model training failed: {train_results['error']}", 'error')
            else:
                r2 = train_results.get('r2', 0)
                mae = train_results.get('mae', 0)
                model_path = train_results.get('model_saved', 'N/A')
                flash(f'âœ… Model training complete! RÂ²: {r2:.4f}, MAE: {mae:.2f}', 'success')
                flash(f'ðŸ“Š {model_path}', 'info')
        except json.JSONDecodeError:
            print("--- [APP] CRITICAL: Could not parse JSON from train_model.py ---")
            flash('Training script ran, but output was not valid JSON. Check logs.', 'error')
    except subprocess.CalledProcessError as e:
        print(f"--- [APP] Training script failed with code {e.returncode} ---")
        print("Script STDOUT:", e.stdout)
        print("Script STDERR:", e.stderr)
        flash(f'Model training failed. See console for details. STDERR: {e.stderr}', 'error')
    except Exception as e:
        print(f"--- [APP] An unexpected error occurred: {e} ---")
        flash(f'An unexpected error occurred: {str(e)}', 'error')
        
    return redirect(url_for('index'))


# ---------------------------------------------------------------
# --- NEW DASHBOARD ROUTES ---
# ---------------------------------------------------------------

@app.route('/dashboard')
def dashboard():
    """Serves the main dashboard HTML page."""
    return render_template('dashboard.html')


@app.route('/api/predict', methods=['POST'])
def api_predict():
    """Handles a single prediction."""
    try:
        # 1. Load model
        model = joblib.load(MODEL_FILE)
        
        # 2. Get data from POST request
        data = request.json
        
        # 3. Create DataFrame (must match the model's training columns)
        X_new = pd.DataFrame({
            "Year": [int(data['year'])],
            "Month": [int(data['month'])],
            "Consultation_Type": [int(data['consult_type'])],
            "Case": [int(data['case_id'])],
            "Sex": [int(data['sex'])],
            "Age_range": [int(data['age_range'])]
        })
        
        # 4. Predict
        prediction = model.predict(X_new)
        
        return jsonify({'prediction': round(prediction[0], 2)})
        
    except FileNotFoundError:
        return jsonify({'error': f'Model file not found at {MODEL_FILE}. Please train the model first.'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/past_data')
def api_past_data():
    """Gets recent past data and historical trend."""
    try:
        df = pd.read_csv(CLEANED_CSV)
        
        # 1. Get last 20 rows for the table
        table_data = df.tail(20).to_dict('records')
        
        # 2. Get historical trend for line chart
        # We group by Year and Month to ensure correct time order
        df['Date'] = pd.to_datetime(df['Year'].astype(str) + '-' + df['Month'].astype(str) + '-01')
        chart_data = df.groupby('Date')['Total'].sum().reset_index()

        return jsonify({
            'table': table_data,
            'chart_data': {
                'labels': chart_data['Date'].dt.strftime('%Y-%m').tolist(),
                'data': chart_data['Total'].tolist()
            }
        })
    except FileNotFoundError:
        return jsonify({'error': f'Data file not found at {CLEANED_CSV}. Please process files first.'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/available_years')
def api_available_years():
    """
    Gets the range of years available in the data, plus 5 future years.
    """
    try:
        df = pd.read_csv(CLEANED_CSV)
        if df.empty:
            # Default if no data: current year +/- 5
            current_year = pd.Timestamp.now().year
            years = list(range(current_year - 5, current_year + 6)) # e.g., 2020 -> 2030
        else:
            min_year = int(df['Year'].min())
            max_year = int(df['Year'].max())
            # Create a list from the min data year up to max data year + 5
            years = list(range(min_year, max_year + 6)) # +6 to be inclusive
        
        # Return years in descending order for the dropdown
        return jsonify({'years': sorted(years, reverse=True)})
        
    except FileNotFoundError:
        # Fallback if no CSV exists yet
        current_year = pd.Timestamp.now().year
        years = list(range(current_year - 5, current_year + 6)) # e.g., 2020 to 2030
        return jsonify({'years': sorted(years, reverse=True)})
    except Exception as e:
        return jsonify({'error': str(e)}), 500
    
@app.route('/api/forecast', methods=['POST'])
def api_forecast():
    """
    Compares actual vs. predicted for a selected year.
    If a future year is selected, it generates a forecast 
    using the most recent year's data as a template.
    """
    try:
        year = int(request.json['year'])
        
        # 1. Load model and full data
        model = joblib.load(MODEL_FILE)
        df = pd.read_csv(CLEANED_CSV)
        
        # Get the most recent year of data you have
        max_data_year = df['Year'].max()
        
        # --- Define month labels for Chart.js ---
        months = range(1, 13)
        month_labels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
        
        # --- SCENARIO 1: Back-testing a past or current year ---
        # (This part is unchanged)
        if year <= max_data_year:
            df_year = df[df['Year'] == year].copy()
            if df_year.empty:
                return jsonify({'error': f'No data found for year {year}.'}), 404
                
            X_year = df_year[["Year", "Month", "Consultation_Type", "Case", "Sex", "Age_range"]]
            df_year['Predicted'] = model.predict(X_year)
            
            actual_by_month = df_year.groupby('Month')['Total'].sum()
            predicted_by_month = df_year.groupby('Month')['Predicted'].sum()

            return jsonify({
                'labels': month_labels,
                'actual': [int(actual_by_month.get(m, 0)) for m in months],
                'predicted': [round(predicted_by_month.get(m, 0), 2) for m in months]
            })

        # --- SCENARIO 2: Forecasting a future year ---
        # (This logic is new)
        else:
            # 1. Use the most recent year's data as a template
            df_template = df[df['Year'] == max_data_year].copy()
            
            # 2. Set the 'Year' to the future year you want to predict
            df_template['Year'] = year 
            
            # 3. Create the feature set (X) from this template
            #    We must drop 'Total' if it exists, as it's not a feature
            features = ["Year", "Month", "Consultation_Type", "Case", "Sex", "Age_range"]
            X_future = df_template[features]
            
            # 4. Predict on this *realistic* set of future data
            X_future['Predicted'] = model.predict(X_future)
            
            # 5. Aggregate by month
            predicted_by_month = X_future.groupby('Month')['Predicted'].sum()
            
            # 6. Format for Chart.js (Actual is 0)
            return jsonify({
                'labels': month_labels,
                'actual': [0] * 12, # No actual data for the future
                'predicted': [round(predicted_by_month.get(m, 0), 2) for m in months]
            })

    except FileNotFoundError:
        return jsonify({'error': 'Model or data file not found. Please train model and process files.'}), 500
    except Exception as e:
        print(f"Error in /api/forecast: {str(e)}") 
        return jsonify({'error': str(e)}), 500

@app.route('/api/top_cases', methods=['POST'])
def api_top_cases():
    """Gets top 10 cases for a selected month."""
    try:
        month = int(request.json['month'])
        
        # 1. Load data and case dictionary
        df = pd.read_csv(CLEANED_CSV)
        with open(CASE_DICT_FILE, 'r') as f:
            case_dict = json.load(f)
        
        # Invert dictionary to map ID -> Name
        inv_case_dict = {str(v): k for k, v in case_dict.items()}
        # The Case column in CSV is numeric, so keys must be numbers
        inv_case_dict_numeric = {v: k for k, v in case_dict.items()}

        
        # 2. Filter by month and get top 10
        df_month = df[df['Month'] == month]
        if df_month.empty:
            return jsonify({'error': f'No data found for month {month}.'}), 404
            
        top_10 = df_month.groupby('Case')['Total'].sum().nlargest(10).reset_index()
        
        # 3. Map Case IDs back to names
        top_10['CaseName'] = top_10['Case'].map(inv_case_dict_numeric).fillna('Unknown Case')
        
        # 4. Format for table and chart
        table_data = top_10.to_dict('records')
        chart_labels = top_10['CaseName'].tolist()
        chart_data = top_10['Total'].tolist()
        
        return jsonify({
            'table': table_data,
            'chart_data': {
                'labels': chart_labels,
                'data': chart_data
            }
        })
        
    except FileNotFoundError:
        return jsonify({'error': 'Data or case dictionary file not found. Please process files.'}), 500
    except Exception as e:
        print(f"Error in /api/top_cases: {e}")
        return jsonify({'error': str(e)}), 500


if __name__ == '__main__':
    app.run(debug=True)
