# D:\tesFolder\app.py
import os
import sys
import subprocess
import pandas as pd
from flask import Flask, render_template, request, redirect, url_for, flash
from werkzeug.utils import secure_filename


# --- Configuration ---
# This path MUST match the one in your pipeline.py
BASE_FOLDER = r"D:\tesFolder"
INPUT_FOLDER = os.path.join(BASE_FOLDER, "data", "excel_folder")
PROCESSED_FOLDER = os.path.join(BASE_FOLDER, "data", "processed")
LOGS_FOLDER = os.path.join(BASE_FOLDER, "logs")

# Files generated by the pipeline
CLEANED_CSV = os.path.join(PROCESSED_FOLDER, "master_dataset_cleaned.csv")
EXCEL_LOG = os.path.join(LOGS_FOLDER, "converted_files.txt")

ALLOWED_EXTENSIONS = {'xlsx'}

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = INPUT_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max upload
app.secret_key = 'your-secret-key-goes-here' # Change this

# --- Helper Functions ---

def allowed_file(filename):
    """Checks if the file extension is allowed (Excel only)"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def get_dashboard_stats():
    """Calculates the stats for the dashboard"""
    stats = {
        'total_files': 0,
        'total_records': 0,
        'success_rate': 100.0, # Default to 100; we only log successes
        'processing': 0  # Hardcoded as we don't have background tasks
    }

    # Ensure folders exist
    os.makedirs(INPUT_FOLDER, exist_ok=True)
    os.makedirs(PROCESSED_FOLDER, exist_ok=True)
    os.makedirs(LOGS_FOLDER, exist_ok=True)

    # 1. Get processed file count from the log
    processed_count = 0
    try:
        if os.path.exists(EXCEL_LOG):
            with open(EXCEL_LOG, "r") as f:
                processed_count = len(f.read().splitlines())
        stats['total_files'] = processed_count
    except Exception:
        stats['total_files'] = 0

    # 2. Total Records: Count rows in the final cleaned CSV
    try:
        df = pd.read_csv(CLEANED_CSV)
        stats['total_records'] = len(df)
    except (FileNotFoundError, pd.errors.EmptyDataError):
        stats['total_records'] = 0
    except Exception as e:
        print(f"Error reading cleaned CSV: {e}")
        stats['total_records'] = 0

    # 3. Success Rate is now simpler.
    if stats['total_files'] == 0:
        stats['success_rate'] = 0.0
    else:
        stats['success_rate'] = 100.0

    return stats

# --- Flask Routes ---

@app.route('/')
def index():
    """Renders the main dashboard page with updated stats."""
    stats = get_dashboard_stats()
    return render_template('index.html', stats=stats)


@app.route('/upload', methods=['POST'])
def upload_file():
    """Handles file upload, saves the file, and triggers the pipeline."""
    
    files = request.files.getlist('file')
    description = request.form.get('description', 'No description') 

    if not files or files[0].filename == '':
        flash('No selected files', 'error')
        return redirect(url_for('index'))

    # 1. Load the log of already processed files
    processed_files_log = set()
    try:
        if os.path.exists(EXCEL_LOG):
            with open(EXCEL_LOG, "r") as f:
                processed_files_log = set(line.strip() for line in f.readlines())
    except Exception as e:
        print(f"Warning: Could not read log file {EXCEL_LOG}. {e}")

    new_files_saved = []
    skipped_files_duplicate = []
    invalid_type_files = []

    # 2. Categorize and save new files
    for file in files:
        filename = secure_filename(file.filename)
        
        if filename == '':
            continue

        if not allowed_file(filename):
            invalid_type_files.append(filename)
            continue

        if filename in processed_files_log:
            skipped_files_duplicate.append(filename)
            continue
            
        try:
            save_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            file.save(save_path)
            new_files_saved.append(filename)
        except Exception as e:
            print(f"Error saving file {filename}: {e}")
            invalid_type_files.append(f"{filename} (Save Error)")

    # 3. Run pipeline AND model training
    pipeline_error = None
    model_error = None
    model_stats = {}

    if new_files_saved:
        try:
            # --- RUN DATA PIPELINE (STEP 1-4) ---
            print("--- [APP] Running data pipeline ---")
            pipeline_path = os.path.join(BASE_FOLDER, 'pipeline.py')
            proc_env = os.environ.copy()
            proc_env['PYTHONIOENCODING'] = 'utf-8'

            result = subprocess.run(
                [sys.executable, pipeline_path],
                capture_output=True, text=True, check=True,
                encoding='utf-8', env=proc_env
            )
            print("--- [APP] Pipeline STDOUT:", result.stdout)
            print("--- [APP] Pipeline STDERR:", result.stderr)
            
            # --- RUN MODEL TRAINING ---
            # This runs directly in the app process
            print("--- [APP] Pipeline complete. Starting model training. ---")
            print("--- [APP] Model training complete. ---")
            
        except subprocess.CalledProcessError as e:
            print(f"--- [APP] Pipeline failed with code {e.returncode} ---")
            print("Pipeline STDOUT:", e.stdout)
            print("Pipeline STDERR:", e.stderr)
            pipeline_error = f'Data pipeline failed. See console.'
        except Exception as e:
            print(f"--- [APP] Model training failed: {e} ---")
            model_error = f'Model training failed: {e}'

    # 4. Flash detailed feedback to the user
    if new_files_saved and not pipeline_error and not model_error and model_stats:
        flash(f'Successfully processed {len(new_files_saved)} new file(s).', 'success')
        # Add model stats to flash
        r2_val = model_stats.get('r2', 0)
        pred_val = model_stats.get('prediction_2025', 0)
        flash(f'New model trained! RÂ²: {r2_val:.4f} | 2025 Predicted Patients: {pred_val:,.0f}', 'success')

    elif pipeline_error:
        flash(f'Uploaded {len(new_files_saved)} file(s), but the pipeline failed: {pipeline_error}', 'error')
    elif model_error:
        flash(f'Data pipeline complete, but model training failed: {model_error}', 'error')
    
    if skipped_files_duplicate:
        flash(f'Skipped {len(skipped_files_duplicate)} file(s) (already processed): {", ".join(skipped_files_duplicate)}', 'warning')
        
    if invalid_type_files:
        flash(f'Skipped {len(invalid_type_files)} file(s) (invalid type or save error).', 'warning')

    if not new_files_saved and not skipped_files_duplicate and not invalid_type_files:
        flash('No files were selected.', 'error')
    elif not new_files_saved:
        flash('No new files to process.', 'success')
            
    return redirect(url_for('index'))

if __name__ == '__main__':
    app.run(debug=True)